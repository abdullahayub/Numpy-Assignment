{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e28a9334-9b88-4d49-8626-cd28aef92b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a,b):\n",
    "    result=a+b\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d0464b9-420e-42cd-b9fb-d3bcd10f43cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Idrees\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenizer\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9799cd2f-04a0-4767-86a2-9b6b5dd2df9e",
   "metadata": {},
   "source": [
    "## NLTK WORL AND SENTENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ab34113-9ca7-4938-a17f-e744610b27fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['God', 'is', 'Great', '!', 'I', 'won', 'a', 'lottery', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "import numpy as np\n",
    "text = \"God is Great! I won a lottery.\"\n",
    "print(word_tokenize(text))\n",
    "\n",
    "#Output: ['God', 'is', 'Great', '!', 'I', 'won', 'a', 'lottery', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54442c5b-ffea-4938-8bf5-51519708a7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Idrees\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "362d2d11-e1e3-4ea7-8271-cd9268d23970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A complete sentence has at, least a subject and a main verb.', 'to state (declare) a complete thought?', 'Short example: Walker walks.', 'A subject is the!', 'noun that is doing the main verb, The main verb is the verb that the subject is doing.', 'In English and many other languages?', 'the first word of a written sentence has a capital letter.', 'At the end of the sentence there is a!', \"full stop or full point (American: 'period').\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "text=\"A complete sentence has at, least a subject and a main verb. to state (declare) a complete thought? Short example: Walker walks. A subject is the! noun that is doing the main verb, The main verb is the verb that the subject is doing. In English and many other languages? the first word of a written sentence has a capital letter. At the end of the sentence there is a! full stop or full point (American: 'period').\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55ae7027-aeed-4d60-9fd4-edea4c56b219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A complete sentence has at',\n",
       " ' least a subject and a main verb. to state (declare) a complete thought? Short example: Walker walks. A subject is the! noun that is doing the main verb',\n",
       " \" The main verb is the verb that the subject is doing. In English and many other languages? the first word of a written sentence has a capital letter. At the end of the sentence there is a! full stop or full point (American: 'period').\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=str(text)\n",
    "a.split(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4ae6d9-e31e-4315-b0a4-ca28413610b9",
   "metadata": {},
   "source": [
    "## POS (PARTS OF SPEECH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53026d56-f305-4dbf-99ee-b632e3ab4738",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Idrees\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0e4d9b-3d25-4864-a47c-00189f290c99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440aac42-cfef-4068-b01e-aa06b1b4cdc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Split: ['learn', 'php', 'from', 'guru99', 'and', 'make', 'study', 'easy']\n",
      "After Token: [('learn', 'JJ'), ('php', 'NN'), ('from', 'IN'), ('guru99', 'NN'), ('and', 'CC'), ('make', 'VB'), ('study', 'NN'), ('easy', 'JJ')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk learn/JJ)\n",
      "  (mychunk php/NN)\n",
      "  from/IN\n",
      "  (mychunk guru99/NN and/CC)\n",
      "  make/VB\n",
      "  (mychunk study/NN easy/JJ))\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import RegexpParser\n",
    "text =\"learn php from guru99 and make study easy\".split()\n",
    "print(\"After Split:\",text)\n",
    "tokens_tag = pos_tag(text)\n",
    "print(\"After Token:\",tokens_tag)\n",
    "patterns= \"\"\"mychunk:{<NN.?>*<VBD.?>*<JJ.?>*<CC>?}\"\"\"\n",
    "chunker = RegexpParser(patterns)\n",
    "print(\"After Regex:\",chunker)\n",
    "output = chunker.parse(tokens_tag)\n",
    "print(\"After Chunking\",output)\n",
    "output.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9d8ed3-0763-4124-a09b-2f8c9be9162f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "text = \"learn php from guru99\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(tokens)\n",
    "tag = nltk.pos_tag(tokens)\n",
    "print(tag)\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "cp  =nltk.RegexpParser(grammar)\n",
    "result = cp.parse(tag)\n",
    "print(result)\n",
    "result.draw()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6c8f38-77d0-417c-88e4-04865eb2bfef",
   "metadata": {},
   "source": [
    "## TWEET TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a515f3f9-4010-4d08-9b76-bcfca90ae3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "s0 = \"This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
    "tknzr.tokenize(s0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af111d56-2395-48ad-b4f5-35096c1edfa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f83d94f3-9430-4e67-991c-1526e2fa0b36",
   "metadata": {},
   "source": [
    "## Stemming Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79feaca-b686-43d1-8fc2-0ed74114fa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "e_words= [\"wait\", \"waiting\", \"waited\", \"waits\"]\n",
    "ps =PorterStemmer()\n",
    "for w in e_words:\n",
    "    rootWord=ps.stem(w)\n",
    "    print(rootWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5864a5c6-be61-478a-b9cd-7cad1cfb679a",
   "metadata": {},
   "source": [
    "## Lemmatization tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe640171-3687-4d3d-a33b-f562f7d8a691",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization is the algorithmic process of finding the lemma of a word depending on their meaning.\n",
    "# It helps in returning the base or dictionary form of a word, which is known as the lemma.\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer  = PorterStemmer()\n",
    "text = \"studies studying cries cry\"\n",
    "tokenization = nltk.word_tokenize(text)\n",
    "for w in tokenization:\n",
    "    print(\"Stemming for {} is {}\".format(w,porter_stemmer.stem(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf813beb-b83f-4088-917f-49e38b8d4374",
   "metadata": {},
   "source": [
    "## extra regexp tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60797e8f-0fab-4388-bcf7-442d2e434c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53061646-be6e-425b-aedd-323f842602b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=\" I won't let you do what came here to do so you shan't do that, we won't let you do it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2ea96d-ac9c-4137-b087-d446b82f1027",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenize(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b1ab41-4517-4d80-ae28-7aad15a9cf7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
